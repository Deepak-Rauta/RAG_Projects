{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with multi data source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access tools like wikipedia, pdf, arxiv we need wrapper, here is the stp by step how to make the wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wikipedia Tools ##\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wikipedia\n"
     ]
    }
   ],
   "source": [
    "api_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=250)\n",
    "wiki = WikipediaQueryRun(api_wrapper=api_wrapper)\n",
    "print(wiki.name)\n",
    "\n",
    "# WikipediaQueryRun:- Is responsibe for running this api_wrapper\n",
    "# WikipediaAPIWrapper:- is communicating with the API wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method VectorStore.as_retriever of <langchain_community.vectorstores.faiss.FAISS object at 0x0000025F6194A020>>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"GOOGLE_API_KEY\"\n",
    "\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/\")\n",
    "docs = loader.load()\n",
    "# Split the documents into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "# Initialize the embeddings\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "vector_db = FAISS.from_documents(documents, embeddings)\n",
    "retriver = vector_db.as_retriever\n",
    "retriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a retriver tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "retrival_tool = create_retriever_tool(retriver, \"langsmith_search\", \"search for information about langsmith\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'langsmith_search'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrival_tool.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arxiv'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Arxiv Tools ##\n",
    "from langchain_community.utilities import ArxivAPIWrapper\n",
    "from langchain_community.tools import ArxivQueryRun\n",
    "\n",
    "arxiv_wrapper=ArxivAPIWrapper(top_k_results=1, doc_content_chars_max=200)\n",
    "arxiv=ArxivQueryRun(api_wrapper=arxiv_wrapper)\n",
    "arxiv.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [wiki, arxiv, retrival_tool]\n",
    "\n",
    "# When ever user make a query first it goes to wiki then it retive the info. then it goes to arxiv, then it goes to retrivel_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from 'c:\\\\Users\\\\kdeepak_new\\\\anaconda3\\\\envs\\\\myr\\\\lib\\\\site-packages\\\\wikipedia\\\\__init__.py'>, top_k_results=1, lang='en', load_all_available_meta=False, doc_content_chars_max=250)),\n",
       " ArxivQueryRun(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=1, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=200)),\n",
       " Tool(name='langsmith_search', description='search for information about langsmith', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x0000025F6C12E560>, retriever=<bound method VectorStore.as_retriever of <langchain_community.vectorstores.faiss.FAISS object at 0x0000025F6194A020>>, document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x0000025F6C44E0E0>, retriever=<bound method VectorStore.as_retriever of <langchain_community.vectorstores.faiss.FAISS object at 0x0000025F6194A020>>, document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'))]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm doing well, thank you for asking! How are you today?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\") \n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "llm = GoogleGenerativeAI(model='gemini-1.5-pro', temperature=0)\n",
    "response = llm.invoke(\"Hi how are you?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Gemini AI is a Google-based model, there isn’t a direct LangChain Hub prompt like OpenAI’s \"hwchase17/openai-functions-agent\". Instead, you need to manually create a prompt template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create our own custom prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"I'm doing well, thank you for asking! As a large language model, I don't experience emotions or feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How can I help you today?\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-fe8e0b8a-03bf-4aab-ade9-1be7ac18d80c-0' usage_metadata={'input_tokens': 24, 'output_tokens': 52, 'total_tokens': 76, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Create an improved prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful and friendly AI assistant. Provide thoughtful, informative answers to user queries.\"),\n",
    "    (\"human\", \"{user_input}\")\n",
    "])\n",
    "\n",
    "# Format the prompt with a user input\n",
    "formatted_prompt = prompt.invoke({\"user_input\": \"Hi, how are you?\"})\n",
    "\n",
    "# Get response from LLM\n",
    "response = llm.invoke(formatted_prompt)\n",
    "\n",
    "# Print response\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More casual/chatty:\n",
    "# \"You are a friendly AI assistant. Keep responses warm, engaging, and conversational.\"\n",
    "\n",
    "# More professional:\n",
    "# \"You are a highly knowledgeable AI. Provide precise, well-structured answers in a professional tone.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code defines and executes a LangChain agent using Google's Gemini model, allowing it to call external tools when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `SampleTool` with `Test Input`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mTool executed successfully with input: Test Input\u001b[0m\u001b[32;1m\u001b[1;3mTool executed successfully with input: Test Input\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'user_input': \"Use the SampleTool with input 'Test Input'\", 'output': 'Tool executed successfully with input: Test Input'}\n"
     ]
    }
   ],
   "source": [
    "# Create the agent\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "from langchain.tools import Tool\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "\n",
    "# Define a sample tool\n",
    "def sample_tool(input_text: str):\n",
    "    return f\"Tool executed successfully with input: {input_text}\"\n",
    "\n",
    "tools = [Tool(name=\"SampleTool\", func=sample_tool, description=\"A sample tool for testing.\")]\n",
    "\n",
    "# ✅ Updated Prompt (Forcing the AI to use tools)\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant. Use tools when necessary to answer user queries.\"),\n",
    "    (\"human\", \"{user_input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")  # Required for agent execution\n",
    "])\n",
    "\n",
    "# Create the agent\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "\n",
    "# ✅ Wrap agent in `AgentExecutor` to ensure it actually calls tools\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "# Run the agent with a direct tool-invoking request\n",
    "response = agent_executor.invoke({\"user_input\": \"Use the SampleTool with input 'Test Input'\"})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentExecutor(verbose=True, agent=RunnableMultiActionAgent(runnable=RunnableAssign(mapper={\n",
       "  agent_scratchpad: RunnableLambda(lambda x: message_formatter(x['intermediate_steps']))\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['agent_scratchpad', 'user_input'], input_types={'agent_scratchpad': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x0000025F6BE37F40>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful AI assistant. Use tools when necessary to answer user queries.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['user_input'], input_types={}, partial_variables={}, template='{user_input}'), additional_kwargs={}), MessagesPlaceholder(variable_name='agent_scratchpad')])\n",
       "| RunnableBinding(bound=ChatGoogleGenerativeAI(model='models/gemini-1.5-pro', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x0000025F6194B250>, default_metadata=()), kwargs={'tools': [{'type': 'function', 'function': {'name': 'SampleTool', 'description': 'A sample tool for testing.', 'parameters': {'properties': {'__arg1': {'title': '__arg1', 'type': 'string'}}, 'required': ['__arg1'], 'type': 'object'}}}]}, config={}, config_factories=[])\n",
       "| ToolsAgentOutputParser(), input_keys_arg=[], return_keys_arg=[], stream_runnable=True), tools=[Tool(name='SampleTool', description='A sample tool for testing.', func=<function sample_tool at 0x0000025F5E388310>)])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To run the agent we need agent excecutor\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "agent_executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mLangSmith is a platform built for debugging, testing, evaluating, and monitoring LLM applications.  It helps developers and researchers close the gap between LLM experimentation and production. Key features include:\n",
      "\n",
      "* **Dataset Management & Analysis:** Create and manage datasets for training and evaluation, and analyze dataset characteristics to understand potential biases and areas for improvement.\n",
      "* **Tracing & Debugging:** Gain deep visibility into the execution flow of your LLM application, enabling step-by-step debugging and analysis of prompts, completions, and intermediate data.\n",
      "* **Evaluation & Monitoring:** Define custom metrics and benchmarks to rigorously evaluate LLM performance, and continuously monitor deployed applications to identify regressions and performance drifts.\n",
      "* **Experiment Tracking:** Track and compare different LLM configurations, prompts, and training parameters, making it easier to identify optimal settings and reproduce successful outcomes.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'user_input': 'Tell me about Langsmith',\n",
       " 'output': 'LangSmith is a platform built for debugging, testing, evaluating, and monitoring LLM applications.  It helps developers and researchers close the gap between LLM experimentation and production. Key features include:\\n\\n* **Dataset Management & Analysis:** Create and manage datasets for training and evaluation, and analyze dataset characteristics to understand potential biases and areas for improvement.\\n* **Tracing & Debugging:** Gain deep visibility into the execution flow of your LLM application, enabling step-by-step debugging and analysis of prompts, completions, and intermediate data.\\n* **Evaluation & Monitoring:** Define custom metrics and benchmarks to rigorously evaluate LLM performance, and continuously monitor deployed applications to identify regressions and performance drifts.\\n* **Experiment Tracking:** Track and compare different LLM configurations, prompts, and training parameters, making it easier to identify optimal settings and reproduce successful outcomes.'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"user_input\": \"Tell me about Langsmith\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mLangSmith is a platform built by Google AI for debugging, testing, evaluating, and monitoring LLM workflows.  It helps developers and researchers close the gap between LLM experimentation and production.  There are a number of helpful features, such as:\n",
      "\n",
      "* **Dataset Management**: Easily manage your datasets for training, evaluating, and testing your LLM applications.\n",
      "* **Evaluation**: Evaluate and compare different LLM configurations on various metrics.\n",
      "* **Debugging**: Dive deep into individual conversations to understand and fix issues with your LLM's behavior.\n",
      "* **Monitoring**: Track the performance of your LLM application over time.  This is important for catching regressions.\n",
      "\n",
      "If you'd like to learn more, you can check out the LangSmith documentation.  I can also answer more specific questions you may have about LangSmith.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'user_input': 'Tell me about Langsmith', 'output': \"LangSmith is a platform built by Google AI for debugging, testing, evaluating, and monitoring LLM workflows.  It helps developers and researchers close the gap between LLM experimentation and production.  There are a number of helpful features, such as:\\n\\n* **Dataset Management**: Easily manage your datasets for training, evaluating, and testing your LLM applications.\\n* **Evaluation**: Evaluate and compare different LLM configurations on various metrics.\\n* **Debugging**: Dive deep into individual conversations to understand and fix issues with your LLM's behavior.\\n* **Monitoring**: Track the performance of your LLM application over time.  This is important for catching regressions.\\n\\nIf you'd like to learn more, you can check out the LangSmith documentation.  I can also answer more specific questions you may have about LangSmith.\"}\n"
     ]
    }
   ],
   "source": [
    "response = agent_executor.invoke({\"user_input\": \"Tell me about Langsmith\"})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI am sorry, I cannot access external websites or specific PDF files, including arXiv papers. Therefore, I cannot provide information about the content of the paper 1605.08386.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'user_input': \"What's the paper 1605.08386 about?\",\n",
       " 'output': 'I am sorry, I cannot access external websites or specific PDF files, including arXiv papers. Therefore, I cannot provide information about the content of the paper 1605.08386.'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"user_input\": \"What's the paper 1605.08386 about?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `ArxivSearch` with `1605.08386`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mPublished: 2016-05-26\n",
      "Title: Heat-bath random walks with Markov bases\n",
      "Authors: Caprice Stanley, Tobias Windisch\n",
      "Summary: Graphs on lattice points are studied whose edges come from a finite set of\n",
      "allowed moves of arbitrary length. We show that the diameter of these graphs on\n",
      "fibers of a fixed integer matrix can be bounded from above by a constant. We\n",
      "then study the mixing behaviour of heat-bath random walks on these graphs. We\n",
      "also state explicit conditions on the set of moves so that the heat-bath random\n",
      "walk, a generalization of the Glauber dynamics, is an expander in fixed\n",
      "dimension.\u001b[0m\u001b[32;1m\u001b[1;3mThe paper \"Heat-bath random walks with Markov bases\" by Caprice Stanley and Tobias Windisch was published on 2016-05-26.  The summary is as follows: \"Graphs on lattice points are studied whose edges come from a finite set of allowed moves of arbitrary length. We show that the diameter of these graphs on fibers of a fixed integer matrix can be bounded from above by a constant. We then study the mixing behaviour of heat-bath random walks on these graphs. We also state explicit conditions on the set of moves so that the heat-bath random walk, a generalization of the Glauber dynamics, is an expander in fixed dimension.\"\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'user_input': 'Find details about paper 1605.08386 on arXiv.', 'output': 'The paper \"Heat-bath random walks with Markov bases\" by Caprice Stanley and Tobias Windisch was published on 2016-05-26.  The summary is as follows: \"Graphs on lattice points are studied whose edges come from a finite set of allowed moves of arbitrary length. We show that the diameter of these graphs on fibers of a fixed integer matrix can be bounded from above by a constant. We then study the mixing behaviour of heat-bath random walks on these graphs. We also state explicit conditions on the set of moves so that the heat-bath random walk, a generalization of the Glauber dynamics, is an expander in fixed dimension.\"'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.tools import Tool\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.utilities import ArxivAPIWrapper  # Arxiv search utility\n",
    "\n",
    "# Define an arXiv research tool\n",
    "arxiv_tool = Tool(\n",
    "    name=\"ArxivSearch\",\n",
    "    func=ArxivAPIWrapper().run,  # This uses LangChain's arXiv API wrapper\n",
    "    description=\"Search for research papers on arXiv using paper ID or keywords.\"\n",
    ")\n",
    "\n",
    "# Define the agent prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant. Use tools when necessary to answer user queries.\"),\n",
    "    (\"human\", \"{user_input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "])\n",
    "\n",
    "# Create an agent with the arXiv tool\n",
    "agent = create_tool_calling_agent(llm, [arxiv_tool], prompt)\n",
    "\n",
    "# Wrap the agent in an executor\n",
    "agent_executor = AgentExecutor(agent=agent, tools=[arxiv_tool], verbose=True)\n",
    "\n",
    "# Run the agent to fetch a research paper summary\n",
    "response = agent_executor.invoke({\"user_input\": \"Find details about paper 1605.08386 on arXiv.\"})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
